# A independent C++ inference pipeline without DataFlow for single Conv2d layer with OpenSource library (DNNL)

## C++ Script Inference using oneDNN Library  
The repo contains Independent CPP inference with oneDNN Library for Convolution layer.

## Machine Requirements:
- Processor Architecture: x86
- RAM: Minimum 8GB
- OS: Ubuntu 20.04 
- Storage: Minimum 64GB

## Prequisites:
- oneDNN Library [documentation](https://github.com/oneapi-src/oneAPI-samples/tree/master/Libraries/oneDNN)
- cmake version 3.29.3
- g++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
- Python 3.8.19 (create virtual enviroment)

## Setting Up the Environment

Before running the project, you need to activate the Python virtual environment.

### Activate the Virtual Environment

```
source myenv/bin/activate
```

### Installing Dependencies

After activating the environment, install the necessary dependencies:

```
pip install -r requirements.txt
```

# Install Prequisites

 Build the oneDNN library by referring to the [documentation](https://oneapi-src.github.io/oneDNN/dev_guide_build.html)

## Cloning the Repo 
Clone the repo using the following command  
```
git clone https://github.com/prachi-mcw/dnnl_conv_inference.git
cd dnnl_conv_inference
```  

## How  Run Python Convolution Layer Inference (For Verification Of CPP Output)


Run the python script to get convolution layer output and dump it to output file
```
python conv_inference.py
```

Note: Execute the Python Inference before running this section
## To Build and Run the CPP Convolution Layer Inference 
  
1. Compile the C++ script 
```
g++ conv2d.cpp -ldnnl -lcurl -lz -o mnist_conv -I<PATH_ON_ONEDNN> 
```
Replace <PATH_ON_ONEDNN> with the actual path of the oneDNN library folder

2. Run the executable file  created by above command 
```
./mnist_conv
```

## Comparing Outputs 
- To Compare the outputs of c++ inference pipeline and python script use compare.py file 

- output are stored in .bin files separately for both c++ and python scripts 
```
python compare.py
```

## Sample Output 

Are the arrays equal? Yes
Maximum absolute difference: 0.000000

Python output (sample):
[[0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.19353707 0.21079427 0.20188104
  0.20849843 0.18659963 0.2092184  0.21211433 0.19553284 0.19716346
  0.20671046 0.20188104 0.20840351 0.1928074  0.20671046 0.20188105
  0.20849843 0.18659963 0.2092184  0.21211433 0.19553284 0.20940787]
 [0.21323414 0.19996485 0.21256207 0.20046762 0.6103059  0.6127655
  0.61557287 0.4052843  0.81364316 0.815224   0.41165933 0.4173409
  0.6096512  0.6185821  0.6089325  0.4166096  0.613735   0.6185821
  0.60879225 0.4226144  0.8127142  0.80697495 0.42606652 0.39126742]]

C++ output (sample):
[[0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147
  0.20578147 0.20578147 0.20578147 0.20578147 0.20578147 0.20578147]
 [0.20578147 0.20578147 0.20578147 0.19353707 0.21079427 0.20188104
  0.20849843 0.18659963 0.2092184  0.21211433 0.19553284 0.19716346
  0.20671046 0.20188104 0.20840351 0.1928074  0.20671046 0.20188104
  0.20849843 0.18659963 0.2092184  0.21211433 0.19553284 0.20940787]]











